{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third-party imports\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Column, Integer, Date, create_engine, inspect, event\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# Local application imports\n",
    "from wifor_db import _env_cache, open_log, close_log\n",
    "\n",
    "class TABLE_CONNECTOR:\n",
    "    def __init__(self):\n",
    "        self.log = open_log(\"CONNECTOR_LOG\")\n",
    "        self.Base = declarative_base()\n",
    "        self.engine = None\n",
    "        self.session = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.log.info(\"OPEN CONNECTOR LOG\")\n",
    "        self.engine = self.create_engine_from_env()\n",
    "        self.session = self.create_session(self.engine)\n",
    "        self.register_before_flush_event(self.session)\n",
    "        self.log.info(\"session created\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "            self.log.info(\"session closed\")\n",
    "        if self.log:\n",
    "            self.log.info(\"CLOSE CONNECTOR LOG\")\n",
    "            close_log(self.log)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_engine_from_env():\n",
    "        current_db = _env_cache['CURRENT_DB']\n",
    "        if current_db == 'sqlite':\n",
    "            db_url = _env_cache['SQLITE_DB_PATH']\n",
    "        elif current_db == 'mysql':\n",
    "            db_url = f\"mysql+pymysql://{_env_cache['MYSQL_DB_USER']}:{_env_cache['MYSQL_DB_PASSWORD']}@{_env_cache['MYSQL_DB_HOST']}/{_env_cache['MYSQL_DB_NAME']}\"\n",
    "        elif current_db == 'postgresql':\n",
    "            db_url = f\"postgresql://{_env_cache['POSTGRES_DB_USER']}:{_env_cache['POSTGRES_DB_PASSWORD']}@{_env_cache['POSTGRES_DB_HOST']}:{_env_cache['POSTGRES_DB_PORT']}/{_env_cache['POSTGRES_DB_NAME']}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported database type: {current_db}\")\n",
    "        return create_engine(db_url)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_session(engine):\n",
    "        if not engine:\n",
    "            raise ValueError(\"Engine not initialized\")\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        return Session()\n",
    "\n",
    "##############################################################################################################\n",
    "    @staticmethod\n",
    "    def load_class_json(self, class_name):\n",
    "        json_path = os.path.join(_env_cache['CLASS_DIR'], f\"{class_name}.json\")\n",
    "        with open(json_path, 'r', encoding=\"utf-8\") as file:\n",
    "            self.log.info(\"\"\"open json in path:\n",
    "                           %s\"\"\", json_path)\n",
    "            return json.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_type(self, type_str):\n",
    "        if '(' in type_str:\n",
    "            base_type, params = type_str.split('(')\n",
    "            param = int(params.rstrip(')'))\n",
    "            return getattr(sqlalchemy, base_type)(param)\n",
    "        return getattr(sqlalchemy, type_str)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_repr_string(self, name, columns):\n",
    "        self.log.info(\"\"\"create repr string from\n",
    "                      name: %s\n",
    "                      columns: %s\"\"\", name, columns)\n",
    "        repr_parts = [f\"{column['name']}='{{self.{column['name']}}}'\" if 'String' in column['type'] else f\"{column['name']}={{self.{column['name']}}}\" for column in columns]\n",
    "        standard_parts = [\"version_number={self.version_number}\", \"effective_date='{self.effective_date}'\", \"expiry_date='{self.expiry_date}'\"]\n",
    "        return f\"<{name}(\" + ', '.join(repr_parts + standard_parts) + \")>\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_class_schema(self, json_data):\n",
    "        attrs = {'__tablename__': json_data['table_name'],\n",
    "                 '__table_args__': {'extend_existing': True},\n",
    "                 '__unique_identifier__': json_data['identifier'],\n",
    "                 '__column_names__': [column['name'] for column in json_data[\"columns\"]],\n",
    "                 'id': Column(Integer, primary_key=True, autoincrement=True)}\n",
    "        \n",
    "        # Add dynamic __repr__ method\n",
    "        repr_string = self.create_repr_string(self, attrs[\"__tablename__\"], json_data['columns'])\n",
    "        attrs['__repr__'] = lambda self: repr_string.format(self=self)\n",
    "\n",
    "        for col in json_data['columns']:\n",
    "            column_type = self.parse_type(self, col['type'])\n",
    "            attrs[col['name']] = Column(column_type)\n",
    "\n",
    "        attrs['version_number'] = Column(Integer, default=1)\n",
    "        attrs['effective_date'] = Column(Date, default=datetime.now)\n",
    "        attrs['expiry_date'] = Column(Date, default=None)\n",
    "\n",
    "        return attrs\n",
    "    \n",
    "########################################################################################################################\n",
    "    \n",
    "    def update_entries(self, previous_entry, new_entry):\n",
    "        \"\"\"Update the expiry_date of the previous entry.\"\"\"\n",
    "        self.log.info(\"Update previous entry: %s and new entry: %s\", previous_entry, new_entry)\n",
    "        previous_entry.expiry_date = datetime.now() - timedelta(days=1)\n",
    "        new_entry.version_number = previous_entry.version_number + 1\n",
    "\n",
    "    def bulk_check_existing_entries(self, session, cls, new_entries):\n",
    "        # Extract unique identifiers for all new entries\n",
    "        unique_ids = [getattr(instance, cls.__unique_identifier__) for instance in new_entries]\n",
    "\n",
    "        # Query the database for these identifiers\n",
    "        existing_entries = session.query(cls).filter(\n",
    "            getattr(cls, cls.__unique_identifier__).in_(unique_ids)\n",
    "        ).all()\n",
    "\n",
    "        # Convert existing entries to a set for easy lookup\n",
    "        existing_set = set(getattr(entry, cls.__unique_identifier__) for entry in existing_entries)\n",
    "\n",
    "        # Return a set of instances that are duplicates\n",
    "        return {instance for instance in new_entries if getattr(instance, cls.__unique_identifier__) in existing_set}\n",
    "    \n",
    "    def bulk_check_previous_versions(self, session, cls, new_entries):\n",
    "        # Extract unique identifiers for all new entries\n",
    "        unique_ids = [getattr(instance, cls.__unique_identifier__) for instance in new_entries]\n",
    "\n",
    "        # Query the database for entries with expiry_date None\n",
    "        previous_versions = session.query(cls).filter(\n",
    "            getattr(cls, cls.__unique_identifier__).in_(unique_ids),\n",
    "            cls.expiry_date.is_(None)\n",
    "        ).all()\n",
    "\n",
    "        # Map unique identifiers to previous version instances\n",
    "        previous_versions_map = {getattr(entry, cls.__unique_identifier__): entry for entry in previous_versions}\n",
    "\n",
    "        return previous_versions_map\n",
    "\n",
    "    def register_before_flush_event(self, session):\n",
    "        @event.listens_for(session, \"before_flush\")\n",
    "        def before_flush(session, flush_context, instances):\n",
    "            self.log.info(\"Before flush event triggered\")\n",
    "\n",
    "            new_entries_by_class = defaultdict(list)\n",
    "            for instance in session.new:\n",
    "                new_entries_by_class[type(instance)].append(instance)\n",
    "\n",
    "            for cls, new_entries in new_entries_by_class.items():\n",
    "                existing_entries = self.bulk_check_existing_entries(session, cls, new_entries)\n",
    "                previous_versions = self.bulk_check_previous_versions(session, cls, new_entries)\n",
    "\n",
    "                for instance in new_entries:\n",
    "                    if instance in existing_entries:\n",
    "                        self.log.info(f\"Duplicate entry found for {instance}. It will not be added to the database.\")\n",
    "                        session.expunge(instance)\n",
    "                    elif getattr(instance, cls.__unique_identifier__) in previous_versions:\n",
    "                        previous_entry = previous_versions[getattr(instance, cls.__unique_identifier__)]\n",
    "                        self.log.info(f\"Previous version exists for {instance}. Updating entries.\")\n",
    "                        self.update_entries(previous_entry, instance)\n",
    "\n",
    "    def process_new_entries_for_class(self, session, cls, new_entries):\n",
    "        # Bulk check for existing entries and previous versions\n",
    "        existing_entries = self.bulk_check_existing_entries(session, cls, new_entries)\n",
    "        previous_versions = self.bulk_check_previous_versions(session, cls, new_entries)\n",
    "\n",
    "        for instance in new_entries:\n",
    "            if instance in existing_entries:\n",
    "                session.expunge(instance)\n",
    "            elif instance in previous_versions:\n",
    "                self.update_entries(previous_versions[instance], instance)\n",
    "\n",
    "    def add_class_methods(self, cls):\n",
    "        session = self.session\n",
    "\n",
    "        @classmethod\n",
    "        def init_table(cls):\n",
    "            engine = session.get_bind()\n",
    "            if not inspect(engine).has_table(cls.__tablename__):\n",
    "                cls.metadata.create_all(engine)\n",
    "\n",
    "        cls.init_table = init_table\n",
    "\n",
    "        @classmethod\n",
    "        def add_data(cls, data):\n",
    "            filtered_data = data[cls.__column_names__]\n",
    "            for _, row in filtered_data.iterrows():\n",
    "                instance = cls(**row.to_dict())\n",
    "                session.add(instance)\n",
    "            session.commit()\n",
    "\n",
    "        cls.add_data = add_data\n",
    "\n",
    "    def open_table(self, class_name):\n",
    "        json_data = self.load_class_json(self, class_name)\n",
    "        class_attrs = self.create_class_schema(self, json_data)\n",
    "        dynamic_class = type(json_data['table_name'], (self.Base,), class_attrs)\n",
    "\n",
    "        self.add_class_methods(dynamic_class)\n",
    "\n",
    "        return dynamic_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HenryZeheWifOR\\AppData\\Local\\Temp\\ipykernel_19224\\1016885848.py:21: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  self.Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "geo_df = gpd.read_file('../geo_data/ref-nuts-2021/NUTS_RG_01M_2021_4326.geojson')\n",
    "geo_df.rename(columns={'NUTS_ID': 'nuts_id'\n",
    "                       , 'LEVL_CODE': 'levl_code'\n",
    "                       , 'CNTR_CODE': 'cntr_code'\n",
    "                       , 'NAME_LATN': 'name_latin'\n",
    "                       , 'NUTS_NAME': 'nuts_name'\n",
    "                       , 'MOUNT_TYPE': 'mount_type'\n",
    "                       , 'URBN_TYPE': 'urban_type'\n",
    "                       , 'COAST_TYPE': 'coast_type'\n",
    "                       , 'FID': 'fid'\n",
    "                       }\n",
    "                , inplace=True)\n",
    "\n",
    "geo_df = geo_df[geo_df['nuts_id'].str.contains(\"AT\", case=False)]\n",
    "\n",
    "with TABLE_CONNECTOR() as tc:\n",
    "    regions = tc.open_table(\"REGIONS\")\n",
    "    regions.init_table()\n",
    "    regions.add_data(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eurostat\n",
    "import pandas as pd\n",
    "\n",
    "df = eurostat.get_data_df(\"lfst_r_lfe2en2\", False)\n",
    "df.rename(columns={'geo\\\\TIME_PERIOD': 'nuts_id'}, inplace=True)\n",
    "df = df.melt(id_vars=['freq', 'nace_r2', 'age', 'sex', 'unit', 'nuts_id'], \n",
    "                  var_name='year', \n",
    "                  value_name='employment')\n",
    "df['year'] = pd.to_datetime(df['year'], format='%Y')\n",
    "\n",
    "df = df[df['nuts_id'].str.contains(\"AT11\", case=False)]\n",
    "\n",
    "with TABLE_CONNECTOR() as tc:\n",
    "    regions = tc.open_table(\"lfst_r_lfe2en2\")\n",
    "    regions.init_table()\n",
    "    regions.add_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wifor-platform-kFL7Vwem-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
